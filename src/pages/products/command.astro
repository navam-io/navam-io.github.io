---
import Layout from '@layouts/Layout.astro';
import Header from '@components/Header.vue';
import Footer from '@components/Footer.vue';
import Button from '@components/ui/Button.vue';

const productSchema = {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "Command",
  "applicationCategory": "DeveloperApplication",
  "description": "AI-powered terminal productivity tool. Access 15+ LLMs from 7 providers directly in your command line with intelligent workflows and markdown automation.",
  "operatingSystem": ["Windows", "macOS", "Linux"],
  "softwareVersion": "1.0.0",
  "datePublished": "2025-10-19",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD",
    "description": "Open source (MIT License). Pay only for LLM API costs (varies by provider)."
  },
  "url": "https://www.navam.io/products/command",
  "downloadUrl": "https://pypi.org/project/command/",
  "codeRepository": "https://github.com/navam-io/command",
  "featureList": [
    "15+ AI models (Claude, GPT-4o, Gemini, Llama, etc.)",
    "7 AI providers (Anthropic, OpenAI, Google, Groq, Perplexity, Ollama, Bedrock)",
    "Markdown workflow automation",
    "Obsidian and VS Code integration",
    "Intent-driven task management",
    "Model testing and comparison",
    "Performance trending visualization",
    "Privacy controls (local or cloud)",
    "Configurable workflows via command.yml",
    "Audit trail of all commands"
  ],
  "softwareRequirements": "Python 3.9+",
  "programmingLanguage": {"@type": "ComputerLanguage", "name": "Python"},
  "license": "https://opensource.org/licenses/MIT",
  "author": {"@type": "Organization", "name": "Navam", "url": "https://www.navam.io"}
};

const howToSchema = {
  "@context": "https://schema.org",
  "@type": "HowTo",
  "name": "How to Install Command",
  "totalTime": "PT1M",
  "step": [
    {"@type": "HowToStep", "position": 1, "name": "Install Command", "text": "pip install command"},
    {"@type": "HowToStep", "position": 2, "name": "Initialize", "text": "cmnd init"},
    {"@type": "HowToStep", "position": 3, "name": "Configure Keys", "text": "Add API keys to .env.local"},
    {"@type": "HowToStep", "position": 4, "name": "Start Using", "text": "ask \"your question\""}
  ]
};

const faqSchema = {
  "@context": "https://schema.org",
  "@type": "FAQPage",
  "mainEntity": [
    {
      "@type": "Question",
      "name": "What is Command?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Command is an AI-powered terminal productivity tool that gives you access to 15+ leading AI models from 7 providers directly in your command line. It includes intelligent workflows, markdown automation, and privacy controls."
      }
    },
    {
      "@type": "Question",
      "name": "Which AI models does Command support?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Command supports Claude (Sonnet 4.5, Haiku 4.5, Opus 4.1), OpenAI (GPT-4o, GPT-4o Mini), Google (Gemini 1.5 Pro/Flash), and local models via Ollama (Llama 3.1, Mistral NeMo, Gemma 2, Qwen 2). Total of 15+ models across 7 providers."
      }
    },
    {
      "@type": "Question",
      "name": "Can I use Command completely offline?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Yes! Command supports Ollama for fully local execution with zero external API calls. Run models like Llama 3.1, Mistral NeMo, Gemma 2, or Qwen 2 entirely on your laptop with complete privacy."
      }
    },
    {
      "@type": "Question",
      "name": "How does Command integrate with Obsidian and VS Code?",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "Command works seamlessly with markdown content. You can create custom workflows in command.yml that reference markdown files in your Obsidian vault or VS Code workspace. Use the 'refer' command to process markdown content with AI models."
      }
    }
  ]
};
---

<Layout
  title="Command - AI-Powered Terminal Productivity with 15+ Models"
  description="Access Claude, GPT-4o, Gemini, and local models directly in your terminal. 15+ models, 7 providers, markdown automation, Obsidian/VS Code integration. Open source, MIT licensed."
  keywords="AI terminal, CLI AI tool, command line AI, Obsidian AI, markdown automation, local LLMs, Ollama, developer productivity"
  ogImage="/images/blog/command-hero.png"
  ogType="product"
>
  <script type="application/ld+json" set:html={JSON.stringify(productSchema)} />
  <script type="application/ld+json" set:html={JSON.stringify(howToSchema)} />
  <script type="application/ld+json" set:html={JSON.stringify(faqSchema)} />

  <Header client:load />

  <main class="min-h-screen">
    <section class="relative py-20 bg-gradient-to-br from-green-900 via-teal-900 to-blue-900 overflow-hidden">
      <div class="absolute inset-0 bg-black/20"></div>
      <div class="container mx-auto px-4 relative z-10">
        <div class="max-w-4xl mx-auto text-center text-white">
          <h1 class="text-5xl md:text-7xl font-black mb-6 leading-tight">
            Command Is All You Need
            <span class="block mt-2 bg-gradient-to-r from-green-200 to-blue-200 bg-clip-text text-transparent">
              15+ Models in Your Terminal
            </span>
          </h1>
          <p class="text-xl md:text-2xl mb-8 text-white/90">
            Access Claude, GPT-4o, Gemini, and local models directly from your command line.
            <strong>No browser tabs, no context switching</strong>‚Äîjust pure workflow.
          </p>
          <div class="flex flex-col sm:flex-row gap-4 justify-center">
            <Button client:load variant="primary" size="lg" href="https://github.com/navam-io/command">
              View on GitHub
            </Button>
            <Button client:load variant="secondary" size="lg" href="#installation">
              Get Started
            </Button>
          </div>
        </div>
      </div>
    </section>

    <section class="py-20 bg-white">
      <div class="container mx-auto px-4">
        <h2 class="text-4xl md:text-5xl font-black text-center mb-16">Why Command?</h2>
        <div class="grid md:grid-cols-3 gap-8 max-w-6xl mx-auto">
          <div class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition">
            <h3 class="text-2xl font-bold mb-3">ü§ñ Latest Models Built-in</h3>
            <p class="mb-4">Access the newest Claude 4.5, GPT-4o, and Gemini 1.5 models with simple aliases.</p>
            <ul class="text-sm text-gray-600 space-y-1">
              <li>‚Ä¢ Claude Sonnet 4.5, Haiku 4.5, Opus 4.1</li>
              <li>‚Ä¢ OpenAI GPT-4o, GPT-4o Mini</li>
              <li>‚Ä¢ Google Gemini 1.5 Pro/Flash</li>
            </ul>
          </div>

          <div class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition">
            <h3 class="text-2xl font-bold mb-3">‚ö° Effortless Setup</h3>
            <p class="mb-4">Get started in 60 seconds with simple .env.local configuration.</p>
            <div class="bg-gray-900 text-green-400 p-3 rounded font-mono text-xs">
              pip install command<br/>
              cmnd init<br/>
              ask "Hello!"
            </div>
          </div>

          <div class="bg-white p-6 rounded-lg shadow-lg hover:shadow-xl transition">
            <h3 class="text-2xl font-bold mb-3">üîí Privacy Controls</h3>
            <p class="mb-4">Choose fully local execution or trusted cloud providers.</p>
            <ul class="text-sm text-gray-600 space-y-1">
              <li>‚Ä¢ Fully local with Ollama</li>
              <li>‚Ä¢ Hybrid workflows</li>
              <li>‚Ä¢ Your data, your choice</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="py-20 bg-gray-50">
      <div class="container mx-auto px-4">
        <h2 class="text-4xl md:text-5xl font-black text-center mb-12">Core Features</h2>
        <div class="grid md:grid-cols-2 gap-6 max-w-5xl mx-auto">
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">üìù Markdown Workflows</h3>
            <p>Create custom workflows that process markdown content. Perfect for Obsidian and VS Code users.</p>
          </div>
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">üß™ Model Testing</h3>
            <p>Compare response quality, latency, and cost across all 15+ models with a single command.</p>
          </div>
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">üìä Performance Trending</h3>
            <p>Visualize latency and token trends over time with built-in sparkline charts.</p>
          </div>
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">üéØ Intent-Driven</h3>
            <p>Manage tasks, goals, and plans as markdown outlines. Your intents power your workflow.</p>
          </div>
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">üìú Audit Trail</h3>
            <p>Every command, prompt, and response logged in trail.yml for complete transparency.</p>
          </div>
          <div class="bg-white p-6 rounded-lg shadow">
            <h3 class="text-xl font-bold mb-3">‚öôÔ∏è Configurable</h3>
            <p>Customize everything via command.yml. Define your own workflows, models, and shortcuts.</p>
          </div>
        </div>
      </div>
    </section>

    <section id="installation" class="py-20 bg-white">
      <div class="container mx-auto px-4">
        <div class="max-w-3xl mx-auto">
          <h2 class="text-4xl md:text-5xl font-black text-center mb-12">Installation</h2>
          <div class="bg-gray-50 rounded-2xl p-8">
            <div class="space-y-6">
              <div>
                <h4 class="font-bold text-lg mb-2">1. Install Command</h4>
                <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                  pip install command
                </div>
              </div>

              <div>
                <h4 class="font-bold text-lg mb-2">2. Initialize</h4>
                <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                  cmnd init
                </div>
                <p class="text-gray-600 mt-2 text-sm">Creates command.yml and .env.local.example in current directory</p>
              </div>

              <div>
                <h4 class="font-bold text-lg mb-2">3. Configure API Keys</h4>
                <p class="text-gray-600 mb-2">Copy .env.local.example to .env.local and add your keys:</p>
                <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                  ANTHROPIC_API_KEY=your_key<br/>
                  OPENAI_API_KEY=your_key<br/>
                  GOOGLE_API_KEY=your_key
                </div>
              </div>

              <div>
                <h4 class="font-bold text-lg mb-2">4. Start Using</h4>
                <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                  cmnd id  # Show active provider and model<br/>
                  ask "How old is the oldest pyramid?"
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="py-20 bg-gray-50">
      <div class="container mx-auto px-4">
        <div class="max-w-3xl mx-auto">
          <h2 class="text-4xl md:text-5xl font-black text-center mb-12">FAQ</h2>
          <div class="space-y-6">
            <div class="bg-white p-6 rounded-lg">
              <h3 class="text-xl font-bold mb-3">What is Command?</h3>
              <p class="text-gray-700">Command is an AI-powered terminal productivity tool that gives you access to 15+ leading AI models from 7 providers directly in your command line.</p>
            </div>
            <div class="bg-white p-6 rounded-lg">
              <h3 class="text-xl font-bold mb-3">Which models are supported?</h3>
              <p class="text-gray-700">Claude (Sonnet 4.5, Haiku 4.5, Opus 4.1), OpenAI (GPT-4o, GPT-4o Mini), Google (Gemini 1.5 Pro/Flash), and local models via Ollama (Llama 3.1, Mistral NeMo, Gemma 2, Qwen 2).</p>
            </div>
            <div class="bg-white p-6 rounded-lg">
              <h3 class="text-xl font-bold mb-3">Can I use Command offline?</h3>
              <p class="text-gray-700">Yes! Command supports Ollama for fully local execution with zero external API calls. Complete privacy on your laptop.</p>
            </div>
            <div class="bg-white p-6 rounded-lg">
              <h3 class="text-xl font-bold mb-3">How does it integrate with Obsidian?</h3>
              <p class="text-gray-700">Command processes markdown files seamlessly. Create custom workflows in command.yml that reference your Obsidian vault. Use the 'refer' command to process notes with AI.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="py-20 bg-gradient-to-br from-teal-900 to-blue-900">
      <div class="container mx-auto px-4 text-center text-white">
        <h2 class="text-4xl md:text-5xl font-black mb-6">Transform Your Terminal</h2>
        <p class="text-xl mb-8 max-w-2xl mx-auto">
          Join developers and knowledge workers using Command for AI-powered productivity.
        </p>
        <div class="flex flex-col sm:flex-row gap-4 justify-center">
          <Button client:load variant="primary" size="lg" href="https://github.com/navam-io/command">
            View on GitHub
          </Button>
          <Button client:load variant="secondary" size="lg" href="/blog/introducing-command-ai-productivity-terminal">
            Read the Blog Post
          </Button>
        </div>
      </div>
    </section>
  </main>

  <Footer client:load />
</Layout>
